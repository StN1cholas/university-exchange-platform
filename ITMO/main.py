import streamlit as st
import pandas as pd
import inspect
from collections import namedtuple
import pymorphy2
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# –§–∏–∫—Å –¥–ª—è inspect.getargspec (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
if not hasattr(inspect, 'getargspec'):
    ArgSpec = namedtuple('ArgSpec', 'args varargs keywords defaults')


    def getargspec(func):
        spec = inspect.getfullargspec(func)
        return ArgSpec(spec.args, spec.varargs, spec.varkw, spec.defaults)


    inspect.getargspec = getargspec

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
st.set_page_config(page_title="–ò–º–ø–æ—Ä—Ç —É—á–µ–±–Ω—ã—Ö –ø–ª–∞–Ω–æ–≤ —Å —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–º –∞–Ω–∞–ª–∏–∑–æ–º", layout="wide")
st.title("üìò –ò–º–ø–æ—Ä—Ç, –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —É—á–µ–±–Ω—ã—Ö –¥–∏—Å—Ü–∏–ø–ª–∏–Ω")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
morph = pymorphy2.MorphAnalyzer()

# –°–ø–∏—Å–æ–∫ —Å—Ç–æ–ø-—Å–ª–æ–≤ –¥–ª—è –∏—Å–∫–ª—é—á–µ–Ω–∏—è (–≤–∫–ª—é—á–∞—è "–∏")
STOP_WORDS = {'–∏', '–≤', '–Ω–∞', '—Å', '–ø–æ', '–¥–ª—è', '–∏–∑', '–æ—Ç', '–¥–æ', '–Ω–µ', '–∑–∞', '–∫', '–∞', '–Ω–æ', '–∏–ª–∏'}

# –û–∂–∏–¥–∞–µ–º—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ –∏—Ö —Å–∏–Ω–æ–Ω–∏–º—ã
expected_columns = {
    "duration": ["–¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å", "–ø—Ä–æ–¥–æ–ª–∂–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å", "duration"],
    "field_of_study": ["–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏", "field of study", "—Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ—Å—Ç—å"],
    "language": ["—è–∑—ã–∫ –æ–±—É—á–µ–Ω–∏—è", "language"],
    "min_gpa_required": ["–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –±–∞–ª–ª", "min gpa", "–º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø—Ä–æ—Ö–æ–¥–Ω–æ–π –±–∞–ª–ª"],
    "title": ["–Ω–∞–∑–≤–∞–Ω–∏–µ", "–¥–∏—Å—Ü–∏–ø–ª–∏–Ω–∞", "course title", "title"],
    "study_format": ["—Ñ–æ—Ä–º–∞—Ç –æ–±—É—á–µ–Ω–∏—è", "study format", "—Ñ–æ—Ä–º–∞ –æ–±—É—á–µ–Ω–∏—è"],
    "university_id": ["id —É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç–∞", "—É–Ω–∏–≤–µ—Ä—Å–∏—Ç–µ—Ç", "university id"],
    "competencies": ["–∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è", "competencies"],
    "description": ["–∞–Ω–Ω–æ—Ç–∞—Ü–∏—è", "–æ–ø–∏—Å–∞–Ω–∏–µ", "description"]
}


def semantic_normalize(text):
    """–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ —Å —É–¥–∞–ª–µ–Ω–∏–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤"""
    if not isinstance(text, str):
        return ""

    words = text.split()
    lemmas = []
    for w in words:
        parsed = morph.parse(w)
        if parsed:
            lemma = parsed[0].normal_form
            if lemma not in STOP_WORDS:  # –ò—Å–∫–ª—é—á–∞–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
                lemmas.append(lemma)
    return ' '.join(lemmas)


def parse_file(uploaded_file):
    """–ß—Ç–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
    if uploaded_file.name.endswith('.csv'):
        return pd.read_csv(uploaded_file)
    else:
        return pd.read_excel(uploaded_file)


def match_columns(df):
    """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∫–æ–ª–æ–Ω–æ–∫"""
    mapping = {}
    for key, variants in expected_columns.items():
        found = None
        for col in df.columns:
            col_lower = col.lower()
            if any(v.lower() in col_lower for v in variants):
                found = col
                break
        mapping[key] = found
    return mapping


def selectbox_with_other(label, options, current_val):
    """–í—ã–ø–∞–¥–∞—é—â–∏–π —Å–ø–∏—Å–æ–∫ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –≤–≤–æ–¥–∞ —Å–≤–æ–µ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
    opts = [None] + list(options) + ["–î—Ä—É–≥–æ–µ"]
    if current_val in opts:
        idx = opts.index(current_val)
    else:
        idx = 0
    choice = st.selectbox(label, opts, index=idx)
    if choice == "–î—Ä—É–≥–æ–µ":
        val = st.text_input(f"–í–≤–µ–¥–∏—Ç–µ —Å–≤–æ—ë –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è {label}:")
        return val.strip() if val.strip() else None
    else:
        return choice


# –ó–∞–≥—Ä—É–∑–∫–∞ —ç—Ç–∞–ª–æ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
REFERENCE_FILE = "test_study_plans_100rows_semesters.xlsx"
try:
    df_reference = pd.read_excel(REFERENCE_FILE)
    column_map_ref = match_columns(df_reference)

    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö - —Ç–µ–ø–µ—Ä—å —Ç–æ–ª—å–∫–æ –ø–æ –æ–ø–∏—Å–∞–Ω–∏—é
    df_reference['semantic_text'] = df_reference.apply(
        lambda row: semantic_normalize(row[column_map_ref.get('description')]
                                       if column_map_ref.get('description') in row
                                          and pd.notna(row[column_map_ref.get('description')])
                                       else ""),
        axis=1
    )

except Exception as e:
    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —ç—Ç–∞–ª–æ–Ω–Ω—ã–π —Ñ–∞–π–ª '{REFERENCE_FILE}': {e}")
    st.stop()


# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Sentence-BERT
@st.cache_resource
def load_model():
    return SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')


model = load_model()


# –í—ã—á–∏—Å–ª–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –¥–ª—è —ç—Ç–∞–ª–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
@st.cache_data(show_spinner=False)
def embed_texts(texts):
    return model.encode(texts, convert_to_numpy=True)


reference_embeddings = embed_texts(df_reference['semantic_text'].tolist())

# –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –∑–∞–≥—Ä—É–∑–∫–∏ —Ñ–∞–π–ª–∞
uploaded_file = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ —Ñ–∞–π–ª —Å —É—á–µ–±–Ω—ã–º–∏ –∫—É—Ä—Å–∞–º–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ (.xlsx –∏–ª–∏ .csv)", type=["xlsx", "csv"])

if uploaded_file:
    df = parse_file(uploaded_file)
    st.subheader("–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Å–º–æ—Ç—Ä –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    st.dataframe(df.head())

    column_map = match_columns(df)

    st.subheader("–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏ –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –∏—Å–ø—Ä–∞–≤—å—Ç–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏—è –ø–æ–ª–µ–π")
    cols = st.columns(3)
    for i, key in enumerate(expected_columns):
        with cols[i % 3]:
            column_map[key] = selectbox_with_other(f"–ü–æ–ª–µ –¥–ª—è '{key}'", df.columns, column_map[key])

    if st.button("–ü–æ–¥—Ç–≤–µ—Ä–¥–∏—Ç—å –∏ –ø—Ä–æ–≤–µ—Å—Ç–∏ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ", type="primary"):
        data = []
        semantic_texts = []
        for _, row in df.iterrows():
            def get_val(k):
                val = column_map.get(k)
                if val is None:
                    return None
                if val in df.columns:
                    return row[val]
                else:
                    return val


            # –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ–ø–∏—Å–∞–Ω–∏–µ –¥–ª—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
            desc = get_val("description") if get_val("description") else ""
            desc_norm = semantic_normalize(desc)
            semantic_texts.append(desc_norm)

            entry = {
                "title": get_val("title"),
                "field_of_study": get_val("field_of_study"),
                "description": desc,
                "normalized_description": desc_norm
            }
            data.append(entry)

        df_processed = pd.DataFrame(data)

        # –í—ã–≤–æ–¥ –Ω–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        st.subheader("–ù–æ—Ä–º–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ")
        st.dataframe(df_processed[['title', 'field_of_study', 'normalized_description']])

        st.success("–î–∞–Ω–Ω—ã–µ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω—ã. –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑...")

        # –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        query_embeddings = embed_texts(semantic_texts)

        # –ö–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ –º–µ–∂–¥—É –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º–∏ –∫—É—Ä—Å–∞–º–∏ –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–º–∏
        similarities = cosine_similarity(query_embeddings, reference_embeddings)

        # –°–±–æ—Ä —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ >80%
        results = []
        for i, row in df_processed.iterrows():
            sims = similarities[i]
            top_idxs = sims.argsort()[-3:][::-1]  # –∏–Ω–¥–µ–∫—Å—ã —Ç–æ–ø-3
            for rank, idx_ref in enumerate(top_idxs, 1):
                similarity_percent = round(sims[idx_ref] * 100, 2)
                if similarity_percent > 80:  # –§–∏–ª—å—Ç—Ä –ø–æ —Å—Ö–æ–∂–µ—Å—Ç–∏ >80%
                    results.append({
                        "–ò—Å—Ö–æ–¥–Ω—ã–π –∫—É—Ä—Å": row['title'],
                        "–ù–∞–ø—Ä–∞–≤–ª–µ–Ω–∏–µ": row['field_of_study'],
                        "–≠—Ç–∞–ª–æ–Ω–Ω—ã–π –∫—É—Ä—Å": df_reference.iloc[idx_ref][column_map_ref.get('title')],
                        "–°—Ö–æ–∂–µ—Å—Ç—å": f"{similarity_percent}%",
                        "–û–ø–∏—Å–∞–Ω–∏–µ (–∏—Å—Ö–æ–¥–Ω–æ–µ)": row['description'],
                        "–û–ø–∏—Å–∞–Ω–∏–µ (—ç—Ç–∞–ª–æ–Ω)": df_reference.iloc[idx_ref][column_map_ref.get('description')]
                    })

        # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è (—Å—Ö–æ–∂–µ—Å—Ç—å >80%)")
        if results:
            df_results = pd.DataFrame(results)
            st.dataframe(df_results.sort_values(['–ò—Å—Ö–æ–¥–Ω—ã–π –∫—É—Ä—Å', '–°—Ö–æ–∂–µ—Å—Ç—å'], ascending=[True, False]))
        else:
            st.warning("–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π —Å —Å—Ö–æ–∂–µ—Å—Ç—å—é –±–æ–ª–µ–µ 80%")

        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
        if results:
            st.subheader("–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
            st.write(f"–ù–∞–π–¥–µ–Ω–æ {len(results)} –∑–Ω–∞—á–∏–º—ã—Ö —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π")

            # –ì—Ä–∞—Ñ–∏–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å—Ö–æ–∂–µ—Å—Ç–∏
            similarity_values = [float(r['–°—Ö–æ–∂–µ—Å—Ç—å'].replace('%', '')) for r in results]
            hist_values = pd.cut(similarity_values, bins=[80, 85, 90, 95, 100],
                                 labels=['80-85%', '85-90%', '90-95%', '95-100%'])
            st.bar_chart(hist_values.value_counts().sort_index())